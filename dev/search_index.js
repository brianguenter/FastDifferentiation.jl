var documenterSearchIndex = {"docs":
[{"location":"futurework/#Future-Work","page":"Future work","title":"Future Work","text":"The two biggest limitations of FD are no support for conditionals involving FD variables and code size that scales with number of operations. \n\nThe first can be dealt with by modifying make_function. The new generated code would have three distinct steps:\n\nEvaluate conditionals that involve FD variables and generate a bit vector of the boolean values.\nIndex into a dictionary with the bit vector to see if this combination of conditionals has been seen before. If it has use the previously generated executable.\nIf necessary generate a new executable where the conditional values are all known. Generate the new expression graph and compute and complile the derivative code and cache it.\n\nThe size of the dictionary can be allowed to grow arbitrarily or an LRU type scheme can be used to discard the least recently used executable from the cahce.\n\nIf the set of conditials has good temporal locality then it shouldn't be necessary to recompile very often. The amortized cost should be good.\n\nWhen expression graphs get larger than about 10^5 nodes LLVM compilation time rises rapidly and can be extremely long. Large programs cannot fit in the smallest and fastest caches, so performance will be limited by cache memory bandwidth, because each instruction is executed only once. In the worst case the runtime generated function wouldn't even fit in the L3 cache and performance would be limited by main memory bandwidth.\n\nA method for dealing with excessive code size is loop rerolling. FD essentially unrolls all loops so vector operations become scalar operations. It should be possible to recognize certain common patterns of unrolling and to undo them. Undoing the unrolling all the way back to the original source expression isn't necessary so long as the code size can be substantially reduced.\n\nThe most common pattern that causes code expansion is tensor contraction, which occurs in matrix-vector and matrix-matrix operations. These patterns are simple and should be easily recognized and rerolled. The original expression graph can be annotated with metadata that makes rerolling easier. For example:\n\njulia> A = make_variables(:a,2,2)\n2×2 Matrix{FastDifferentiation.Node}:\n a1_1  a1_2\n a2_1  a2_2\n\njulia> b = make_variables(:b,2)\n2-element Vector{FastDifferentiation.Node}:\n b1\n b2\n\njulia> jacobian(cos.(A*b),vcat(vec(A),b))\n2×6 Matrix{FastDifferentiation.Node}:\n (-(sin(((a1_1 * b1) + (a1_2 * b2)))) * b1)                                         0.0  …  (-(sin(((a1_1 * b1) + (a1_2 * b2)))) * a1_2)\n                                        0.0  (-(sin(((a2_1 * b1) + (a2_2 * b2)))) * b1)     (-(sin(((a2_1 * b1) + (a2_2 * b2)))) * a2_2)\n\njulia> A\n2×2 Matrix{FastDifferentiation.Node}:\n a1_1  a1_2\n a2_1  a2_2\n\njulia> b\n2-element Vector{FastDifferentiation.Node}:\n b1\n b2\n\njulia> A*b\n2-element Vector{Any}:\n ((a1_1 * b1) + (a1_2 * b2))\n ((a2_1 * b1) + (a2_2 * b2))\n\njulia> cos.(A*b)\n2-element Vector{FastDifferentiation.Node{typeof(cos), 1}}:\n cos(((a1_1 * b1) + (a1_2 * b2)))\n cos(((a2_1 * b1) + (a2_2 * b2)))\n\njulia> jacobian(ans,vcat(vec(A),b))\n2×6 Matrix{FastDifferentiation.Node}:\n (-(sin(((a1_1 * b1) + (a1_2 * b2)))) * b1)  0.0                      …                     (-(sin(((a1_1 * b1) + (a1_2 * b2)))) * a1_2)\n0.0                                         (-(sin(((a2_1 * b1) + (a2_2 * b2)))) * b1)      (-(sin(((a2_1 * b1) + (a2_2 * b2)))) * a2_2)\n\nBecause the variable indices are carried in the variable names it should be relatively easy to spot tensor contraction sequences like this (a1_1 * b1) + (a1_2 * b2)) and replace them with a tensor contraction operator on matrix elements.\n\nAnother possibility is to use DynamicExpressions.jl instead of LLVM compilation of large runtime generated programs. When graph size goes above 10^5 nodes LLVM compilation time can increase dramatically, but DynamicExpressions can be quite fast at even larger scales and have reasonable performance. This would be especially useful when the function is changing frequently so compilation overhead cannot be amortized across many derivative evaluations.\n\nSome hybrid of loop rerolling and DynamicExpressions may make it possible to scale FD to expressions several orders of magnitude larger than the current practical limit.\n\nIn the short term reducing memory allocations during the derivative computation stage should substantially improve performance.","category":"section"},{"location":"benchmarks/#Benchmarks","page":"Benchmarks","title":"Benchmarks","text":"See Benchmarks.jl for the benchmark code used to generate these results.\n\nThe benchmarks test the speed of gradients, Jacobians, Hessians, and the ability to exploit sparsity in the derivative. The last problem, ODE, also compares the AD algorithms to a hand optimized Jacobian. There are not many benchmarks so take these results with a grain of salt; they may be useful for order of magnitude comparisons but not much more. Also, two of these packages, FastDifferentiation and Enzyme, are under active development. These benchmarks could change materially in the near future.\n\nI am not an expert in any of these packages except for FD. For some of the benchmarks I have not yet figured out how to correctly and efficiently compute all the derivatives. I am indebted to Yingbo Ma and Billy Moses for their help debugging and improving the benchmark code for ForwardDiff and Enzyme, respectively. \n\nSeveral of the AD algorithms have unexpectedly slow timings; the Enzyme Rosenbrock Hessian timings are notable in this respect since for the other benchmarks Enzyme has excellent performance. Perhaps these codes can be rewritten to be more efficient. If you are expert in any of these packages please submit a PR to fix, improve, or correct a benchmark.\n\nWhen determining which AD algorithm to use keep in mind the limitations of FD: operation count and conditionals. The total operation count of your expression should be less than 10⁵. You may get reasonable performance for expressions as large as 10⁶ operations but expect very long compile times. FD does not support conditionals which involve the differentiation variables (yet). The other algorithms do not have these limitations.","category":"section"},{"location":"benchmarks/#Results","page":"Benchmarks","title":"Results","text":"To generate the markdown for the results in this section execute the function write_markdown() in the file Benchmarks.jl.\n\nThese timings are just for evaluating the derivative function. They do not include preprocessing time required to generate and compile the function nor any time needed to generate auxiliary data structures that make the evaluation more efficient.\n\nThe times in each row are normalized to the shortest time in that row. The fastest algorithm will have a relative time of 1.0 and all other algorithms will have a time ≥ 1.0. Smaller numbers are better.\n\nAll benchmarks run on this system:\n\nJulia Version 1.9.2\nCommit e4ee485e90 (2023-07-05 09:39 UTC)\nPlatform Info:\n  OS: Windows (x86_64-w64-mingw32)\n  CPU: 32 × AMD Ryzen 9 7950X 16-Core Processor            \n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-14.0.6 (ORCJIT, znver3)\n  Threads: 1 on 32 virtual cores\nEnvironment:\n  JULIA_EDITOR = code.cmd\n  JULIA_NUM_THREADS = 1\n\nFunction FD sparse FD dense ForwardDiff ReverseDiff Enzyme Zygote\nRosenbrock Hessian 1.00 15.60 67031.33 193591.71 367.77 163244.34\nRosenbrock gradient [1] 1.13 629.23 281.72 1.00 3967.02\nSimple matrix Jacobian [1] 1.00 41.16 52.39 [50] 123.91\nSpherical harmonics Jacobian [1] 1.00 29.00 [40] [51] [6]\n\n[1]: FD sparse was slower than FD dense so results are only shown for dense.\n\n[1]: FD sparse was slower than FD dense so results are only shown for dense.\n\n[50]: Enzyme prints \"Warning: using fallback BLAS replacements, performance may be degraded\", followed by stack overflow error or endless loop.\n\n[1]: FD sparse was slower than FD dense so results are only shown for dense.\n\n[40]: ReverseDiff failed on Spherical harmonics.\n\n[51]: Enzyme crashes Julia REPL for SHFunctions benchmark.\n\n[6]: Zygote doesn't work with Memoize","category":"section"},{"location":"benchmarks/#Comparison-to-hand-optimized-Jacobian.","page":"Benchmarks","title":"Comparison to hand optimized Jacobian.","text":"This compares AD algorithms to a hand optimized Jacobian (in file ODE.jl). As before timings are relative to the fastest time. Enzyme (array) is written to accept a vector input and return a matrix output to be compatible with the calling convention for the ODE function. This is very slow because Enzyme does not yet do full optimizations on these input/output types. Enzyme (tuple) is written to accept a tuple input and returns tuple(tuples). This is much faster but not compatible with the calling convetions of the ODE function. This version uses features not avaialable in the registered version of Enzyme (as of 7-9-2023). You will need to ] add Enzyme#main instead of using the registered version.\n\nFD sparse FD Dense ForwardDiff ReverseDiff Enzyme (array) Enzyme (tuple) Zygote Hand optimized\n1.00 1.74 29.28 [41] 255.63 4.22 504683.35 2.30\n\nIt is worth nothing that both FD sparse and FD dense are faster than the hand optimized Jacobian. [41]: ODE not implemented for ReverseDiff","category":"section"},{"location":"benchmarks/#Rate-of-growth-of-Jacobian","page":"Benchmarks","title":"Rate of growth of Jacobian","text":"It is also intersting to note the ratio of the number of operations of the FD Jacobian of a function to the number of operations in the original function. \n\nProblem sizes in approximately the ratio 1 \\:10 \\: 100 \\: 1000 were computed for several of the benchmarks.\n\nThe ratio (jacobian operations)/(original function operations) stays close to a constant over 2 orders of magnitude of problem size for Rosenbrock and Spherical harmonics. For the simple matrix ops Jacobian the ratio goes from 2.6 to 6.5 over 3 orders of magnitude of problem size. The ratio is growing far more slowly than the domain and codomain dimensions of the problem: the smallest instance is an R⁸->R⁴ function and the largest is R⁸⁰⁰->R⁴⁰⁰ an increase in both domain and codomain dimensions of 100x.\n\nRelative problem size Rosenbrock Jacobian Spherical harmonics Jacobian Simple matrix ops Jacobian\n1x 1.13 2.2 2.6\n10x 1.13 2.34 3.5\n100x 1.13 2.4 3.8\n1000x   6.5\n\nThis is a very small sample of functions but it will be interesting to see if this slow growth of the Jacobian with increasing domain and codomain dimensions generalizes to all functions or only applies to functions with special graph structure.","category":"section"},{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#FastDifferentiation.@variables-Tuple","page":"API","title":"FastDifferentiation.@variables","text":"@variables args...\n\ncreate FD variables to use in symbolic expressions. Example:\n\njulia> @variables x y\ny\n\njulia> f = x*y\n(x * y)\n\n\n\n\n\n","category":"macro"},{"location":"api/#FastDifferentiation.clear_cache-Tuple{}","page":"API","title":"FastDifferentiation.clear_cache","text":"clear_cache()\n\nClears the global expression cache. To maximize efficiency of expressions the differentation system automatically eliminates common subexpressions by checking for their existence in the global expression cache. Over time this cache can become arbitrarily large. Best practice is to clear the cache before you start defining expressions, define your expressions and then clear the cache.\n\n\n\n\n\n","category":"method"},{"location":"api/#FastDifferentiation.derivative-Tuple{AbstractArray{<:FastDifferentiation.Node}, Vararg{Any}}","page":"API","title":"FastDifferentiation.derivative","text":"derivative(A::AbstractArray{<:Node}, variables...)\n\nComputes ∂A/(∂variables[1],...,∂variables[n]). Repeated differentiation rather than computing different columns of the Jacobian.\n\nExample\n\njulia> A = [t t^2;3t^2 5]  \n2×2 Matrix{Node}:\n t              (t ^ 2)\n (3 * (t ^ 2))  5\n\njulia> derivative(A,t)  \n2×2 Matrix{Node}:\n 1.0      (2 * t)\n (6 * t)  0.0\n\njulia> derivative(A,t,t)  \n2×2 Matrix{Node{T, 0} where T}:\n 0.0  2\n 6    0.0\n\n\n\n\n\n","category":"method"},{"location":"api/#FastDifferentiation.derivative-Tuple{FastDifferentiation.Node, Vararg{Any}}","page":"API","title":"FastDifferentiation.derivative","text":"Convenience derivative for scalar functions. Takes a scalar input and returns a scalar output\n\n\n\n\n\n","category":"method"},{"location":"api/#FastDifferentiation.differential-Tuple{Vararg{FastDifferentiation.Node}}","page":"API","title":"FastDifferentiation.differential","text":"differential(variables::Node...)\n\nReturns an anonymous function that takes the derivative of a scalar function with respect to variables.\n\nExample\n\njulia> @variables t\nt\n\njulia> f = t^2\n(t ^ 2)\n\njulia> Dt = differential(t)      \n#69 (generic function with 1 method)\n\njulia> Dt(f)\n(2 * t)\n\njulia> Dt = differential(t,t)    \n#69 (generic function with 1 method)\n\njulia> Dt(f)\n2\n\n\n\n\n\n","category":"method"},{"location":"api/#FastDifferentiation.hessian-Union{Tuple{S}, Tuple{FastDifferentiation.Node, AbstractVector{S}}} where S<:FastDifferentiation.Node","page":"API","title":"FastDifferentiation.hessian","text":"hessian(expression::Node, variable_order::AbstractVector{<:Node})\n\nReturns the dense symbolic Hessian matrix.\n\nExample\n\njulia> @variables x y\n\njulia> hessian(x^2*y^2,[x,y])\n2×2 Matrix{FastDifferentiation.Node}:\n (2 * (y ^ 2))  (4 * (y * x))\n (4 * (x * y))  (2 * (x ^ 2))\n\n\n\n\n\n","category":"method"},{"location":"api/#FastDifferentiation.hessian_times_v-Union{Tuple{S}, Tuple{T}, Tuple{T, AbstractVector{S}}} where {T<:FastDifferentiation.Node, S<:FastDifferentiation.Node}","page":"API","title":"FastDifferentiation.hessian_times_v","text":"hessian_times_v(term::Node, partial_variables::AbstractVector{<:Node})\n\nComputes Hessian times a vector v without forming the Hessian matrix. Useful when the Hessian would be impractically large.\n\n\n\n\n\n","category":"method"},{"location":"api/#FastDifferentiation.if_else","page":"API","title":"FastDifferentiation.if_else","text":"Special if_else to use for conditionals instead of builtin ifelse because the latter evaluates all its arguments. Many ifelse statements are used as guards against computations that would cause an exception so need a new version that is transformed into \n\ncondition ? true_branch : false_branch\n\nduring code generation.\n\n\n\n\n\n","category":"function"},{"location":"api/#FastDifferentiation.jacobian-Union{Tuple{S}, Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{S}}} where {T<:FastDifferentiation.Node, S<:FastDifferentiation.Node}","page":"API","title":"FastDifferentiation.jacobian","text":"jacobian(\n    terms::AbstractVector{<:Node},\n    partial_variables::AbstractVector{<:Node}\n)\n\nJacobian matrix of the n element function defined by terms. Each term element is a Node expression graph. Only the columns of the Jacobian corresponsing to the elements of partial_variables will be computed and the partial columns in the Jacobian matrix will be in the order specified by partial_variables. Examples:\n\njulia> @variables x y\n\njulia> jacobian([x*y,y*x],[x,y])\n2×2 Matrix{Node}:\n y  x\n y  x\n\njulia> jacobian([x*y,y*x],[y,x])\n2×2 Matrix{Node}:\n x  y\n x  y\n\njulia> jacobian([x*y,y*x],[x])\n2×1 Matrix{Node}:\n y\n y\n\n\n\n\n\n","category":"method"},{"location":"api/#FastDifferentiation.jacobian_times_v-Union{Tuple{S}, Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{S}}} where {T<:FastDifferentiation.Node, S<:FastDifferentiation.Node}","page":"API","title":"FastDifferentiation.jacobian_times_v","text":"jacobian_times_v(\n    terms::AbstractVector{<:Node},\n    partial_variables::AbstractVector{<:Node}\n)\n\nReturns a vector of Node, where each element in the vector is the symbolic form of Jv. Also returns v_vector a vector of the v variables. This is useful if you want to generate a function to evaluate Jv and you want to separate the inputs to the function and the v variables.\n\n\n\n\n\n","category":"method"},{"location":"api/#FastDifferentiation.jacobian_transpose_v-Union{Tuple{S}, Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{S}}} where {T<:FastDifferentiation.Node, S<:FastDifferentiation.Node}","page":"API","title":"FastDifferentiation.jacobian_transpose_v","text":"jacobian_transpose_v(\n    terms::AbstractVector{<:Node},\n    partial_variables::AbstractVector{<:Node}\n)\n\nReturns a vector of Node, where each element in the vector is the symbolic form of Jᵀv. Also returns v_vector a vector of the v variables. This is useful if you want to generate a function to evaluate Jᵀv and you want to separate the inputs to the function and the v variables.\n\n\n\n\n\n","category":"method"},{"location":"api/#FastDifferentiation.make_Expr-Union{Tuple{Ti}, Tuple{T}, Tuple{SparseArrays.SparseMatrixCSC{T, Ti}, Vararg{AbstractVector}}} where {T<:FastDifferentiation.Node, Ti}","page":"API","title":"FastDifferentiation.make_Expr","text":"make_Expr(\n    A::SparseMatrixCSC{<:Node,<:Integer},\n    input_variables::AbstractVector{<:Node},\n    in_place::Bool, init_with_zeros::Bool\n)\n\ninit_with_zeros argument is not used for sparse matrices.\n\n\n\n\n\n","category":"method"},{"location":"api/#FastDifferentiation.make_Expr-Union{Tuple{T}, Tuple{AbstractArray{T}, Vararg{AbstractVector}}} where T<:FastDifferentiation.Node","page":"API","title":"FastDifferentiation.make_Expr","text":"make_Expr(\n    func_array::AbstractArray{<:Node},\n    input_variables::AbstractVector{<:Node},\n    in_place::Bool,\n    init_with_zeros::Bool\n)\n\n\n\n\n\n","category":"method"},{"location":"api/#FastDifferentiation.make_function-Union{Tuple{T}, Tuple{AbstractArray{T}, Vararg{AbstractVector{<:FastDifferentiation.Node}}}} where T<:FastDifferentiation.Node","page":"API","title":"FastDifferentiation.make_function","text":"make_function(\n    func_array::AbstractArray{<:Node},\n    input_variables::AbstractVector{<:Node}...;\n    in_place::Bool=false, init_with_zeros::Bool=true\n)\n\nMakes a function to evaluate the symbolic expressions in func_array. Every variable that is used in func_array must also be in input_variables. However, it will not cause an error if variables in input_variables are not variables used by func_array.\n\njulia> @variables x\nx\n\njulia> f = x+1\n(x + 1)\n\n\njulia> jac = jacobian([f],[x]) #the Jacobian has a single constant element, 1, and is no longer a function of x\n1×1 Matrix{FastDifferentiation.Node}:\n 1\n\n julia> fjac = make_function(jac,[x])\n ...\n \n julia> fjac(2.0) #the value 2.0 is passed in for the variable x but has no effect on the output. Does not cause a runtime exception.\n 1×1 Matrix{Float64}:\n  1.0\n\nIf in_place=false then a new array will be created to hold the result each time the function is called. If in_place=true the function expects a user supplied array to hold the result. The user supplied array must be the first argument to the function.\n\njulia> @variables x\nx\n\njulia> f! = make_function([x,x^2],[x],in_place=true)\n...\n\njulia> result = zeros(2)\n2-element Vector{Float64}:\n 0.0\n 0.0\n\njulia> f!(result,[2.0])\n4.0\n\njulia> result\n2-element Vector{Float64}:\n 2.0\n 4.0\n\nIf the array is sparse then the keyword argument init_with_zeros has no effect. If the array is dense and in_place=true then the keyword argument init_with_zeros affects how the in place array is initialized. If init_with_zeros = true then the in place array is initialized with zeros. If init_with_zeros=false it is the user's responsibility to initialize the array with zeros before passing it to the runtime generated function.\n\nThis can be useful for modestly sparse dense matrices with say at least 1/4 of the array entries non-zero. In this case a sparse matrix may not be as efficient as a dense matrix. But a large fraction of time could be spent unnecessarily setting elements to zero. In this case you can initialize the in place Jacobian array once with zeros before calling the run time generated function.\n\n\n\n\n\n","category":"method"},{"location":"api/#FastDifferentiation.make_variables-Tuple{Symbol, Vararg{Any}}","page":"API","title":"FastDifferentiation.make_variables","text":"makevariables(name::Symbol,arraysize::T...)\n\nReturns an Array of variables with names corresponding to their indices in the Array. \n\nExample:\n\njulia> make_variables(:x,3)\n3-element Vector{FastDifferentiation.Node}:\n x1\n x2\n x3\n\njulia> make_variables(:x,2,3)\n2×3 Matrix{FastDifferentiation.Node}:\n x1_1  x1_2  x1_3\n x2_1  x2_2  x2_3\n\njulia> make_variables(:x,2,3,2)\n2×3×2 Array{FastDifferentiation.Node, 3}:\n[:, :, 1] =\n x1_1_1  x1_2_1  x1_3_1\n x2_1_1  x2_2_1  x2_3_1\n\n[:, :, 2] =\n x1_1_2  x1_2_2  x1_3_2\n x2_1_2  x2_2_2  x2_3_2\n\n\n\n\n\n\n\n","category":"method"},{"location":"api/#FastDifferentiation.sparse_hessian-Union{Tuple{S}, Tuple{FastDifferentiation.Node, AbstractVector{S}}} where S<:FastDifferentiation.Node","page":"API","title":"FastDifferentiation.sparse_hessian","text":"sparse_hessian(expression::Node, variable_order::AbstractVector{<:Node})\n\nCompute a sparse symbolic Hessian. Returns a sparse matrix of symbolic expressions.  Can be used in combination with make_function to generate an executable that  will return a sparse matrix or take one as an in-place argument.\n\nExample\n\njulia> @variables x y\n\njulia> a = sparse_hessian(x*y,[x,y])\n2×2 SparseArrays.SparseMatrixCSC{FastDifferentiation.Node, Int64} with 2 stored entries:\n ⋅  1\n 1  ⋅\n\njulia> f1 = make_function(a,[x,y])\n...\n\njulia> f1([1.0,2.0])\n2×2 SparseArrays.SparseMatrixCSC{Float64, Int64} with 2 stored entries:\n  ⋅   1.0\n 1.0   ⋅\n\njulia> tmp = similar(a,Float64)\n2×2 SparseArrays.SparseMatrixCSC{Float64, Int64} with 2 stored entries:\n  ⋅            4.24399e-314\n 4.24399e-314   ⋅\n\njulia> f2 = make_function(a,[x,y],in_place=true)\n...\n\njulia> f2(tmp, [1.0,2.0])\n2×2 SparseArrays.SparseMatrixCSC{Float64, Int64} with 2 stored entries:\n  ⋅   1.0\n 1.0   ⋅\n\njulia> tmp\n2×2 SparseArrays.SparseMatrixCSC{Float64, Int64} with 2 stored entries:\n  ⋅   1.0\n 1.0   ⋅\n\n\n\n\n\n\n","category":"method"},{"location":"api/#FastDifferentiation.sparse_jacobian-Union{Tuple{S}, Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{S}}} where {T<:FastDifferentiation.Node, S<:FastDifferentiation.Node}","page":"API","title":"FastDifferentiation.sparse_jacobian","text":"sparse_jacobian(\n    terms::AbstractVector{<:Node},\n    partial_variables::AbstractVector{<:Node}\n)\n\nReturns a sparse array containing the Jacobian of the function defined by terms\n\n\n\n\n\n","category":"method"},{"location":"api/#FastDifferentiation.sparsity-Tuple{AbstractArray{<:FastDifferentiation.Node}}","page":"API","title":"FastDifferentiation.sparsity","text":"sparsity(sym_func::AbstractArray{<:Node})\n\nComputes a number representing the sparsity of the array of expressions. If nelts is the number of elements in the array and nzeros is the number of zero elements in the array then sparsity = (nelts-nzeros)/nelts. \n\nFrequently used in combination with a call to make_function to determine whether to set keyword argument init_with_zeros to false.\n\n\n\n\n\n","category":"method"},{"location":"makefunction/#How-to-use-make_function","page":"How to use make_function","title":"How to use make_function","text":"","category":"section"},{"location":"makefunction/#make_function-options","page":"How to use make_function","title":"make_function options","text":"The make_function procedure creates a runtime generated function from FD expressions. The signature for make_function is\n\nmake_function(func_array::AbstractArray{T}, input_variables::AbstractVector{<:Node}...; in_place::Bool=false, init_with_zeros::Bool=true) where {T<:Node}\n\nThe type of the first argument, func_array, determines the type of the array that will hold the result of evaluating the FD expressions. These are rough rules of thumb for parameter settings for maximum performance:\n\nIf func_array is dense and has more than 100 elements then typeof(func_array) should be a subtype of Array with in_place=true, init_with_zeros=false (but see caveat below)\nIf func_array is sparse (use the sparsity function to measure this) then typeof(func_array) should be SparseMatrixCSC (automatically created by sparse_jacobian and sparse_hessian functions) and in_place=true. The init_with_zeros argument has no effect for sparse arrays.\nif func_array is small, less than 100 elements, then typeof(func_array) should be a subtype of StaticArray, in_place=false, init_with_zeros=true.\n\nThe type of the second argument input_variables has no effect on performance of the runtime generated function.\n\nIf the third argument, in_place, = false then the generated code will create and return a zero initialized array at every call. If in_place = true the generated code has two arguments: an array to accept the result of the evaluation, and a vector of inputs. There will only be a performance advantage by setting in_place = true if the type of func_array is Array or SparseMatrixCSC.\n\nThe fourth argument, init_with_zeros, only has an effect when in_place=true and the type of func_array is Array or SArray. If init_with_zeros = true then the in place array argument will be initialized with zeros at each call. If it is false it will not be initialized with zeros. \n\nThis is useful when you are evaluating a function many times but not modifying the result array. You can initialize the in place array with zeros once before passing it to the runtime generated function. The runtime generated function will not alter identically zero elements so they will stay zero. Be careful though that some other function doesn't modify the in place array between calls to the runtime generated function. This could corrupt the identically zero elements.\n\nExample: generated function returns static array\n\n\n@variables x y\n\njulia> func = SA[x^2+y,y*x,y^2]\n3-element SVector{3, FastDifferentiation.Node} with indices SOneTo(3):\n ((x ^ 2) + y)\n       (y * x)\n       (y ^ 2)\n\njulia> f_exe! = make_function(func,[x,y])\n...\n\njulia> f_exe!([2.0,3.0])\n3-element SVector{3, Float64} with indices SOneTo(3):\n 7.0\n 6.0\n 9.0\n\nExample: generated function accepts more than one vector of input arguments. This is useful if you want to segregate your input variables into groups.\n\n    x = make_variables(:x, 3)\n    y = make_variables(:y, 3)\n    f = x .* y\n    f_callable = make_function(f, x, y)\n    x_val = ones(3)\n    y_val = ones(3)\n    f_val = f_callable(x_val, y_val) #executable takes two 3-vectors as input arguments\n\nExample: assume your result is a large dense array (> 100 elements) and that you are using an in place array with no initialization. For dense arrays this should generate the fastest code:\n\njulia> @variables x y z\nz\n\njulia> p = [x^2 y^2; z^2 0]\n2×2 Matrix{FastDifferentiation.Node}:\n (x ^ 2)  (y ^ 2)\n (z ^ 2)        0\n\njulia> floatp = similar(p,Float64)\n2×2 Matrix{Float64}:\n 5.0e-324  1.08279e-311\n 1.5e-323  1.08279e-311\n\njulia> fexe! = make_function(p,[x,y,z],in_place=true,init_with_zeros=false)\n...\n\n\njulia> fexe!(floatp,[1.0,2.0,3.0])\n4.0\n\njulia> floatp\n2×2 Matrix{Float64}:\n 1.0  4.0\n 9.0  1.08279e-311\n\n\nNotice that floatp[2,2] is not 0. This is because init_with_zeros=false. If you need this array element to be zero then initialize it before making the call to fexe!.\n\nYou can use the sparsity function to measure sparsity and determine whether you should use the dense or sparse derivative functions to pass as the func_array argument.","category":"section"},{"location":"makefunction/#Evaluate-a-function-and-derivatives","page":"How to use make_function","title":"Evaluate a function and derivatives","text":"Sometimes you want to evaluate a function and one or more derivative orders. If you pack all the terms you want to evaluate into the argument to make_function then common terms will be detected and only computed once. This will be generally be more efficient than evaluating the function and derivatives separately:\n\njulia> f = [x^2*y^2,sqrt(x*y)]\n2-element Vector{FastDifferentiation.Node}:\n ((x ^ 2) * (y ^ 2))\n       sqrt((x * y))\n\njulia> jac = jacobian(f,[x,y])\n2×2 Matrix{FastDifferentiation.Node}:\n             ((y ^ 2) * (2 * x))              ((x ^ 2) * (2 * y))\n ((1 / (2 * sqrt((x * y)))) * y)  ((1 / (2 * sqrt((x * y)))) * x)\n\n\njulia> f_and_jac = make_function([vec(jac);f],[x,y])\n...\n\njulia> tmp = f_and_jac([1.1,2.1])\n6-element Vector{Float64}:\n 9.702000000000002\n 0.6908492797077573\n 5.082000000000001\n 0.36187343222787294\n 5.336100000000001\n 1.5198684153570665\n\njulia> jac_eval = reshape(view(tmp,1:4),2,2)\n2×2 reshape(view(::Vector{Float64}, 1:4), 2, 2) with eltype Float64:\n 9.702     5.082\n 0.690849  0.361873\n\njulia> f_eval = view(tmp,5:6)\n2-element view(::Vector{Float64}, 5:6) with eltype Float64:\n 5.336100000000001\n 1.5198684153570665","category":"section"},{"location":"makefunction/#Generating-code","page":"How to use make_function","title":"Generating code","text":"If you need to generate code that can be cut and pasted into another application you can use make_Expr instead of make_function. It has the same arguments except the in_place and init_with_zeros boolean arguments are not optional.\n\njulia> @variables x y\ny\n\njulia> j = jacobian([x^2*y^2,cos(x+y),log(x/y)],[x,y])\n3×2 Matrix{FastDifferentiation.Node}:\n       ((y ^ 2) * (2 * x))                 ((x ^ 2) * (2 * y))\n           -(sin((x + y)))                     -(sin((x + y)))\n ((1 / (x / y)) * (1 / y))  ((1 / (x / y)) * -(((x / y) / y)))\n\njulia> in_place_no_init = make_Expr(j,[x,y],true,false)\n:((result, input_variables)->begin\n          #= c:\\Users\\seatt\\source\\FastDifferentiation.jl\\src\\CodeGeneration.jl:127 =#\n          #= c:\\Users\\seatt\\source\\FastDifferentiation.jl\\src\\CodeGeneration.jl:127 =# @inbounds begin\n                  #= c:\\Users\\seatt\\source\\FastDifferentiation.jl\\src\\CodeGeneration.jl:128 =#\n                  begin\n                      var\"##431\" = input_variables[2] ^ 2\n                      var\"##432\" = 2 * input_variables[1]\n                      var\"##430\" = var\"##431\" * var\"##432\"\n                      result[CartesianIndex(1, 1)] = var\"##430\"\n                      var\"##435\" = input_variables[1] + input_variables[2]\n                      var\"##434\" = sin(var\"##435\")\n                      var\"##433\" = -var\"##434\"\n                      result[CartesianIndex(2, 1)] = var\"##433\"\n                      var\"##438\" = input_variables[1] / input_variables[2]\n                      var\"##437\" = 1 / var\"##438\"\n                      var\"##439\" = 1 / input_variables[2]\n                      var\"##436\" = var\"##437\" * var\"##439\"\n                      result[CartesianIndex(3, 1)] = var\"##436\"\n                      var\"##441\" = input_variables[1] ^ 2\n                      var\"##442\" = 2 * input_variables[2]\n                      var\"##440\" = var\"##441\" * var\"##442\"\n                      result[CartesianIndex(1, 2)] = var\"##440\"\n                      result[CartesianIndex(2, 2)] = var\"##433\"\n                      var\"##445\" = var\"##438\" / input_variables[2]\n                      var\"##444\" = -var\"##445\"\n                      var\"##443\" = var\"##437\" * var\"##444\"\n                      result[CartesianIndex(3, 2)] = var\"##443\"\n                  end\n              end\n      end)\n\njulia> in_place_zero_init = make_Expr(j,[x,y],true,true)\n:((result, input_variables)->begin\n          #= c:\\Users\\seatt\\source\\FastDifferentiation.jl\\src\\CodeGeneration.jl:127 =#\n          #= c:\\Users\\seatt\\source\\FastDifferentiation.jl\\src\\CodeGeneration.jl:127 =# @inbounds begin\n                  #= c:\\Users\\seatt\\source\\FastDifferentiation.jl\\src\\CodeGeneration.jl:128 =#\n                  begin\n                      var\"##447\" = input_variables[2] ^ 2\n                      var\"##448\" = 2 * input_variables[1]\n                      var\"##446\" = var\"##447\" * var\"##448\"\n                      result[CartesianIndex(1, 1)] = var\"##446\"\n                      var\"##451\" = input_variables[1] + input_variables[2]\n                      var\"##450\" = sin(var\"##451\")\n                      var\"##449\" = -var\"##450\"\n                      result[CartesianIndex(2, 1)] = var\"##449\"\n                      var\"##454\" = input_variables[1] / input_variables[2]\n                      var\"##453\" = 1 / var\"##454\"\n                      var\"##455\" = 1 / input_variables[2]\n                      var\"##452\" = var\"##453\" * var\"##455\"\n                      result[CartesianIndex(3, 1)] = var\"##452\"\n                      var\"##457\" = input_variables[1] ^ 2\n                      var\"##458\" = 2 * input_variables[2]\n                      var\"##456\" = var\"##457\" * var\"##458\"\n                      result[CartesianIndex(1, 2)] = var\"##456\"\n                      result[CartesianIndex(2, 2)] = var\"##449\"\n                      var\"##461\" = var\"##454\" / input_variables[2]\n                      var\"##460\" = -var\"##461\"\n                      var\"##459\" = var\"##453\" * var\"##460\"\n                      result[CartesianIndex(3, 2)] = var\"##459\"\n                  end\n              end\n      end)","category":"section"},{"location":"symbolicprocessing/#Symbolic-Processing","page":"Symbolic processing","title":"Symbolic Processing","text":"Because FD can generate true symbolic derivatives it can easily be used in conjunction with Symbolics.jl using the package FDConversion.jl (still under development).\n\nA rule of thumb is that if your function is small (a few hundred operations or less) or tree like (where each node in the expression graph has one parent on average) then Symbolics.jl may outperform or equal FD. For more complex functions with many common subexpressions FD may substantially outperform Symbolics.jl.\n\nTake these benchmarks with a large grain of salt since there are so few of them. Whether your function will have this kind of performance improvement relative to Symbolics.jl is hard to predict until the benchmark set gets much bigger.\n\nThese benchmarks should give you a sense of what performance you might achieve for symbolic processing. There are three types of benchmarks: Symbolic, MakeFunction, and Exe.\n\nThe Symbolic benchmark is the time required to compute just the symbolic form of the derivative. The Symbolic benchmark can be run with simplification turned on or off for Symbolics.jl. If simplification is on then computation time can be extremely long but the resulting expression might be simpler and faster to execute.\nThe MakeFunction benchmark is the time to generate a Julia Expr from an already computed symbolic derivative and to then compile it.\nThe Exe benchmark measures just the time required to execute the compiled function using an in-place matrix.\n\nAll benchmarks show the ratio of time taken by Symbolics.jl to FastDifferentiation.jl. Numbers greater than 1 mean FastDifferentiation is faster.\n\nAll benchmarks were run on an AMD Ryzen 9 7950X 16-Core Processor with 32GB RAM running Windows 11 OS, Julia version 1.9.0.","category":"section"},{"location":"symbolicprocessing/#Chebyshev-polynomial","page":"Symbolic processing","title":"Chebyshev polynomial","text":"The first example is a recursive function for  the Chebyshev polynomial of order n:\n\n@memoize function Chebyshev(n, x)\n    if n == 0\n        return 1\n    elseif n == 1\n        return x\n    else\n        return 2 * (x) * Chebyshev(n - 1, x) - Chebyshev(n - 2, x)\n    end\nend\n\nThe function is memoized so the recursion executes efficiently. \n\nThe recursive function returns an nth order polynomial in the variable x. The derivative of this polynomial would be order n-1 so a perfect symbolic simplification would result in a function with 2*(n-2) operations. For small values of n Symbolics.jl simplification does fairly well but larger values result in very inefficient expressions.\n\nBecause FD doesn't do sophisticated symbolic simplification it generates a derivative with approximately 2.4x the number of operations in the original recursive expression regardless of n. This is a case where a good hand generated derivative would be more efficient than FD.\n\nThe Chebyshev expression graph does not have many nodes even at the largest size tested (graph size increases linearly with Chebyshev order).\n\nThe first set of three benchmarks show results with simplification turned off in Symbolics.jl, followed by a set of three with simplification turned on. Performance is somewhat better in the latter case but still slower than the FD executable. Note that the y axis is logarithmic.","category":"section"},{"location":"symbolicprocessing/#Chebyshev-benchmarks-with-simplification-off","page":"Symbolic processing","title":"Chebyshev benchmarks with simplification off","text":"(Image: Symbolic processing, simplify=false)  (Image: MakeFunction, simplify=false)  (Image: Exe, simplify=false)","category":"section"},{"location":"symbolicprocessing/#Chebyshev-benchmarks-with-simplification-on","page":"Symbolic processing","title":"Chebyshev benchmarks with simplification on","text":"(Image: MakeFunction, simplify=false)\n\nWith simplification on performance of the executable derivative function for Symbolics.jl is slightly better than with simplification off. But simplification processing time is longer.","category":"section"},{"location":"symbolicprocessing/#Spherical-Harmonics","page":"Symbolic processing","title":"Spherical Harmonics","text":"The second example is the spherical harmonics function. This is the expression graph for the spherical harmonic function of order 8: (Image: MakeFunction, simplify=false)\n\n@memoize function P(l, m, z)\n    if l == 0 && m == 0\n        return 1.0\n    elseif l == m\n        return (1 - 2m) * P(m - 1, m - 1, z)\n    elseif l == m + 1\n        return (2m + 1) * z * P(m, m, z)\n    else\n        return ((2l - 1) / (l - m) * z * P(l - 1, m, z) - (l + m - 1) / (l - m) * P(l - 2, m, z))\n    end\nend\nexport P\n\n@memoize function S(m, x, y)\n    if m == 0\n        return 0\n    else\n        return x * C(m - 1, x, y) - y * S(m - 1, x, y)\n    end\nend\nexport S\n\n@memoize function C(m, x, y)\n    if m == 0\n        return 1\n    else\n        return x * S(m - 1, x, y) + y * C(m - 1, x, y)\n    end\nend\nexport C\n\nfunction factorial_approximation(x)\n    local n1 = x\n    sqrt(2 * π * n1) * (n1 / ℯ * sqrt(n1 * sinh(1 / n1) + 1 / (810 * n1^6)))^n1\nend\nexport factorial_approximation\n\nfunction compare_factorial_approximation()\n    for n in 1:30\n        println(\"n $n relative error $((factorial(big(n))-factorial_approximation(n))/factorial(big(n)))\")\n    end\nend\nexport compare_factorial_approximation\n\n@memoize function N(l, m)\n    @assert m >= 0\n    if m == 0\n        return sqrt((2l + 1 / (4π)))\n    else\n        # return sqrt((2l+1)/2π * factorial(big(l-m))/factorial(big(l+m)))\n        #use factorial_approximation instead of factorial because the latter does not use Stirlings approximation for large n. Get error for n > 2 unless using BigInt but if use BigInt get lots of rational numbers in symbolic result.\n        return sqrt((2l + 1) / 2π * factorial_approximation(l - m) / factorial_approximation(l + m))\n    end\nend\nexport N\n\n\"\"\"l is the order of the spherical harmonic\"\"\"\n@memoize function Y(l, m, x, y, z)\n    @assert l >= 0\n    @assert abs(m) <= l\n    if m < 0\n        return N(l, abs(m)) * P(l, abs(m), z) * S(abs(m), x, y)\n    else\n        return N(l, m) * P(l, m, z) * C(m, x, y)\n    end\nend\nexport Y\n\nSHFunctions(max_l, x::Node, y::Node, z::Node) = SHFunctions(Vector{Node}(undef, 0), max_l, x, y, z)\nSHFunctions(max_l, x::Symbolics.Num, y::Symbolics.Num, z::Symbolics.Num) = SHFunctions(Vector{Symbolics.Num}(undef, 0), max_l, x, y, z)\n\nfunction SHFunctions(shfunc, max_l, x, y, z)\n    for l in 0:max_l-1\n        for m in -l:l\n            push!(shfunc, Y(l, m, x, y, z))\n        end\n    end\n\n    return shfunc\nend\nexport SHFunctions\n\nfunction spherical_harmonics(::JuliaSymbolics, model_size)\n    Symbolics.@variables x y z\n    return SHFunctions(model_size, x, y, z), [x, y, z]\nend\n\nfunction spherical_harmonics(::FastSymbolic, model_size, x, y, z)\n    graph = DerivativeGraph(SHFunctions(model_size, x, y, z))\n    return graph\nend\n\nfunction spherical_harmonics(package::FastSymbolic, model_size)\n    FD.@variables x, y, z\n    return spherical_harmonics(package, model_size, x, y, z)\nend\nexport spherical_harmonics\n\nAs was the case for Chebyshev polynomials the number of paths from the roots to the variables is much greater than the number of nodes in the graph. Once again the y axis is logarithmic.\n\n(Image: Symbolic processing, simplify=false) (Image: MakeFunction, simplify=false) (Image: Exe, simplify=false)\n\nThe Exe benchmark took many hours to run and was stopped at model size 24 instead of 25 as for the Symbolic and MakeFunction benchmarks.","category":"section"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"FD uses a global cache for common subexpression elimination so the FD symbolics preprocessing step is not thread safe. \n\nUnder ordinary conditions the memory used by the cache won't be an issue. But, if you have a long session where you are creating many complex functions it is possible the cache will use too much memory. If this happens call the function clear_cache after you have completely processed your expression.\n\nThe most common way to use FD is this:\n\ncreate variables\ndo operations on those variables to create the function you want to differentiate\ncompute a symbolic derivative of the function\npass the symbolic derivative to make_function to generate a function to efficiently evaluate the derivative","category":"section"},{"location":"examples/#Creating-Variables","page":"Examples","title":"Creating Variables","text":"Scalar variables\n\nusing FastDifferentiation\n\n@variables x y z\n\n\nArrays of variables of arbitrary dimension\n\njulia> make_variables(:x,3)\n3-element Vector{FastDifferentiation.Node}:\n x1\n x2\n x3\n\njulia> make_variables(:x,2,3)\n2×3 Matrix{FastDifferentiation.Node}:\n x1_1  x1_2  x1_3\n x2_1  x2_2  x2_3\n\njulia> make_variables(:x,2,3,2)\n2×3×2 Array{FastDifferentiation.Node, 3}:\n[:, :, 1] =\n x1_1_1  x1_2_1  x1_3_1\n x2_1_1  x2_2_1  x2_3_1\n\n[:, :, 2] =\n x1_1_2  x1_2_2  x1_3_2\n x2_1_2  x2_2_2  x2_3_2","category":"section"},{"location":"examples/#Compute-derivatives","page":"Examples","title":"Compute derivatives","text":"Compute higher order derivatives\n\njulia> @variables x y\ny\n\njulia> f = x^3*y^3\n((x ^ 3) * (y ^ 3))\n\njulia> derivative([f],x,y,x) #take derivative wrt x, then y, then x\n1-element Vector{FastDifferentiation.Node{typeof(*), 2}}:\n (18 * (x * (y ^ 2)))\n\n julia> derivative([cos(x*y);;;exp(x*y)],x,y,x) #derivative accepts input arrays of any dimension\n1×1×2 Array{FastDifferentiation.Node{typeof(+), 2}, 3}:\n[:, :, 1] =\n ((-(y) * cos((x * y))) + ((((x * -(y)) * -(sin((x * y)))) + -(cos((x * y)))) * y))\n\n[:, :, 2] =\n (((((x * y) + 1) * exp((x * y))) * y) + (y * exp((x * y))))\n\nCompute derivative of a function and make executable\n\n# compute Jacobian and generate function to evaluate it\njulia> f1 = cos(x) * y\n(cos(x) * y)\n\njulia> f2 = sin(y) * x\n(sin(y) * x)\n\njulia> symb = jacobian([f1, f2], [x, y]) #the vector [x,y] tells make_function \n# how to order the arguments to the generated function\n2×2 Matrix{Node}:\n (y * -(sin(x)))  cos(x)\n sin(y)           (x * cos(y))\n\njulia> jac_exe = make_function(symb,[x,y]) \n...\njulia> jac_exe([1.0,2.0]) #jac_exe was created with variable ordering [x,y] \n# so x will get the value 1.0, y 2.0\n2×2 Matrix{Float64}:\n -1.68294    0.540302\n  0.909297  -0.416147\n\n# compute Hessian and generate function to evaluate it\n@variables x y z\n\njulia> h_symb1 = hessian(x^2*y^2*z^2,[x,y,z])\n3×3 Matrix{FastDifferentiation.Node}:\n (2 * ((z ^ 2) * (y ^ 2)))        (((2 * x) * (2 * y)) * (z ^ 2))  (((2 * x) * (2 * z)) * (y ^ 2))\n (((2 * y) * (2 * x)) * (z ^ 2))  (2 * ((z ^ 2) * (x ^ 2)))        (((2 * y) * (2 * z)) * (x ^ 2))\n (((2 * z) * (2 * x)) * (y ^ 2))  (((2 * z) * (2 * y)) * (x ^ 2))  (2 * ((x ^ 2) * (y ^ 2)))\n\njulia> hexe_1 = make_function(h_symb1,[x,y,z]) #the vector [x,y,z] tells make_function \n# how to order the arguments to the generated function\n...\njulia> hexe_1([1.0,2.0,3.0]) #hexe_1 was created with variable ordering [x,y,z] \n# so x will get the value 1.0, y 2.0, and z 3.0\n3×3 Matrix{Float64}:\n 72.0  72.0  48.0\n 72.0  18.0  24.0\n 48.0  24.0   8.0\n\nCompute any subset of the columns of the Jacobian:\n\njulia> symb = jacobian([x*y,y*z,x*z],[x,y,z]) #all columns\n3×3 Matrix{Node}:\n y    x    0.0\n 0.0  z    y\n z    0.0  x\n\njulia> symb = jacobian([x*y,y*z,x*z],[x,y]) #first two columns\n3×2 Matrix{Node}:\n y    x\n 0.0  z\n z    0.0\n\njulia> symb = jacobian([x*y,y*z,x*z],[z,y]) #second and third columns, ordered so ∂f/∂z is 1st column of the output, ∂f/∂y the 2nd\n3×2 Matrix{Node}:\n 0.0  x\n y    z\n x    0.0","category":"section"},{"location":"examples/#Sparse-Jacobians-and-Hessians","page":"Examples","title":"Sparse Jacobians and Hessians","text":"The functions sparse_jacobian, sparse_hessian compute sparse symbolic derivatives. When you pass a sparse symbolic function matrix to make_function it will generate an executable which expects an in place sparse matrix to hold the result. For functions with sparse Jacobians or Hessians this can be orders of magnitude faster than using a dense in place matrix.\n\njulia> hess = sparse_hessian(x^3 + y^3 + z^3, [x,y,z])        \n3×3 SparseArrays.SparseMatrixCSC{FastDifferentiation.Node, Int64} with 3 stored entries:\n (6 * x)        ⋅        ⋅\n       ⋅  (6 * y)        ⋅\n       ⋅        ⋅  (6 * z)\n\n\njulia> res = similar(hess,Float64) #make sparse matrix with proper sparsity to pass to the generated function\n3×3 SparseArrays.SparseMatrixCSC{Float64, Int64} with 3 stored entries:\n 0.0   ⋅    ⋅ \n  ⋅   0.0   ⋅\n  ⋅    ⋅   0.0\n\njulia> sp_f = make_function(hess,[x,y,z])\n...\n\njulia> sp_f([1.0,2.0,3.0],res)\n3×3 SparseArrays.SparseMatrixCSC{Float64, Int64} with 3 stored entries:\n 6.0    ⋅     ⋅\n  ⋅   12.0    ⋅\n  ⋅     ⋅   18.0","category":"section"},{"location":"examples/#Less-commonly-used-functions","page":"Examples","title":"Less commonly used functions","text":"Compute Hv without forming the full Hessian matrix. This is useful if the Hessian is very large\n\njulia> @variables x y\ny\n\njulia> f = x^2 * y^2\n((x ^ 2) * (y ^ 2))\n\njulia> hv_fast, v_vec2 = hessian_times_v(f, [x, y])\n...\n\njulia> hv_fast_exe = make_function(hv_fast, [[x, y]; v_vec2]) #need v_vec2 because hv_fast is a function of x,y,v1,v2 and have to specify the order of all inputs to the executable\n...\njulia> hv_fast_exe([1.0,2.0,3.0,4.0]) #first two vector elements are x,y last two are v1,v2\n2-element Vector{Float64}:\n 56.0\n 32.0\n\nSymbolic and executable Jᵀv and Jv (see this paper for applications of this operation).\n\njulia> (f1,f2) = cos(x)*y,sin(y)*x\n((cos(x) * y), (sin(y) * x))\n\njulia> jv,vvec = jacobian_times_v([f1,f2],[x,y])\n...\n\njulia> jv_exe = make_function(jv,[[x,y];vvec])\n...\n\njulia> jv_exe([1.0,2.0,3.0,4.0]) #first 2 arguments are x,y values and last two are v vector values\n\n2×1 Matrix{Float64}:\n -2.8876166853748195\n  1.0633049342884753\n\njulia> jTv,rvec = jacobian_transpose_v([f1,f2],[x,y])\n...\n\njulia> jtv_exe = make_function(jTv,[[x,y];rvec])\n...\njulia> jtv_exe([1.0,2.0,3.0,4.0])\n2-element Vector{Float64}:\n -1.4116362015446517\n -0.04368042858415033","category":"section"},{"location":"howitworks/#How-it-works","page":"How it works","title":"How it works","text":"FD is a domain specific language (DSL) embedded in Julia. FD defines a custom Number type and nd overloads all the mathematical operators in Base to apply to this new number type. You create FD numbers using either @variables or make_variables.\n\nMathematical operations on FD numbers create a graph representing the mathematical expression rather than immediately returning a floating point value. For example, in this code fragment \n\n@variables x y\nf(a,b)= cos(a)*sin(b)\n\nmyexpr = f(x,y)\n\nmyexpr contains a graph representation of the cos(x)*sin(y) where x,y are FD numbers. \n\nFor the most part there is no difference between using FD numbers and the base number types, Float64, Int64, etc. You define your Julia function as you normally world and then call it with FD numbers as inputs; the return value will be a graph representing the expression your Julia function computes.  \n\nThe FD differentiation functions, jacobian, hessian, etc., take FD expression graphs as inputs and return FD expression graphs. To turn this into executable Julia code you pass an FD expression graph as an argument to make_function.\n\nAll the FD differntiation functions use derivative graph factorization[2] to compute derivatives. The FD differentiation algorithm is related to the D* algorithm but is asymptotically faster so it works on much larger expression graphs. The new algorithms used in FD will be described in a soon to be written paper. FD automatic differentiaion is fundamentally different from forward and reverse automatic differentiation. \n\nThe efficiency of FD comes from analysis of the graph structure of the function rather than sophisticated algebraic simplification rules. By default FD applies only these algebraic simplications[1] to expressions:\n\nx×0=>0\nx×1=>x\nx/1=>x\nx+0=>x\nc₁×c₂=>c₃ for c₁,c₂,c₃ constants\nc₁+c₂=>c₃ for c₁,c₂,c₃ constants\nc₁×(c₂×x) => (c₁×c₂)×x  for c₁,c₂ constants\n\nThese rules are generally safe in the sense of obeying IEEE floating point arithmetic rules. However if the runtime value of x happens to be NaN or Inf the FD expression x*0 will identically return 0, because it will have been rewritten to 0 by the simplification rules. The expected IEEE result is NaN.\n\n[1]: More rules may be added in future versions of FD to improve efficiency.\n\n[2]: See the D*  paper for an explanation of derivative graph factorization. ","category":"section"},{"location":"limitations/#Limitations","page":"Limitations","title":"Limitations","text":"FD does not support looping internally. All operations with loops, such as matrix vector multiplication, are unrolled into scalar operations. The corresponding executable functions generated by make_function have size proportional to the number of operations. \n\nExpressions with ≈10⁵ scalar operations have reasonable symbolic preprocessing and compilation times.  Beyond this size LLVM compilation time can become extremely long and eventually the executables become so large that their caching behavior is not good and performance declines.","category":"section"},{"location":"#Introduction","page":"Introduction","title":"Introduction","text":"(Image: Build Status)\n\nFastDifferentiation (FD) is a package for generating efficient executables to evaluate derivatives of Julia functions. It can also generate efficient true symbolic derivatives for symbolic analysis. Unlike forward and reverse mode automatic differentiation FD automatically generates efficient derivatives for arbitrary function types: ℝ¹->ℝ¹, ℝ¹->ℝᵐ, ℝⁿ->ℝ¹, and ℝⁿ->ℝᵐ. \n\nFor f:ℝⁿ->ℝᵐ with n,m large FD may have better performance than conventional AD algorithms because the FD algorithm finds expressions shared between partials and computes them only once. In some cases FD derivatives can be as efficient as manually coded derivatives (see the Lagrangian dynamics example in the D* paper or the Benchmarks section of the documentation for another example).\n\nFD may take much less time to compute symbolic derivatives than Symbolics.jl even in the ℝ¹->ℝ¹ case. The executables generated by FD may also be much faster (see Symbolic Processing). \n\nYou should consider using FastDifferentiation when you need: \n\na fast executable for evaluating the derivative of a function and the overhead of the preprocessing/compilation time is swamped by evaluation time.\nto do additional symbolic processing on your derivative. FD can generate a true symbolic derivative to be processed further in Symbolics.jl or another computer algebra system.\n\nThis is the FD feature set:\n\n Dense Jacobian Sparse Jacobian Dense Hessian Sparse Hessian Higher order derivatives Jᵀv Hv\nCompiled function ✅ ✅ ✅ ✅ ✅ ✅ ✅\nSymbolic expression ✅ ✅ ✅ ✅ ✅ ✅ ✅\n\nJᵀv and Jv compute the Jacobian transpose times a vector and the Jacobian times a vector, without explicitly forming the Jacobian matrix. For applications see this paper. \n\nHv computes the Hessian times a vector without explicitly forming the Hessian matrix. This can be useful when the Hessian matrix is large and sparse.\n\nIf you use FD in your work please share the functions you differentiate with me. I'll add them to the benchmarks. The more functions available to test the easier it is for others to determine if FD will help with their problem.\n\nThis is beta software being modified on a daily basis. Expect bugs and frequent, possibly breaking changes, over the next month or so. Documentation is frequently updated so check the latest docs before filing an issue. Your problem may have been fixed and documented.","category":"section"},{"location":"#Notes-about-special-derivatives","page":"Introduction","title":"Notes about special derivatives","text":"The derivative of |u| is u/|u| which is NaN when u==0. This is not a bug. The derivative of the absolute value function is undefined at 0 and the way FD signals this is by returning NaN.","category":"section"},{"location":"#Conditionals","page":"Introduction","title":"Conditionals","text":"As of version 0.4.1 FD allows you to create expressions with conditionals using either the builtin ifelse function or a new function if_else. ifelse will evaluate both inputs. By contrast if_else has the semantics of if...else...end; only the true or false branch will be executed. This is useful when your conditional is used to prevent exceptions because of illegal input values:\n\njulia> f = if_else(x<0,NaN,sqrt(x))\n(if_else  (x < 0) NaN sqrt(x))\n\njulia> g = make_function([f],[x])\n\n\njulia> g([-1])\n1-element Vector{Float64}:\n NaN\n\njulia> g([2.0])\n1-element Vector{Float64}:\n 1.4142135623730951\nend\n\nIn this case you wouldn't want to use ifelse because it evaluates both the true and false branches and causes a runtime exception:\n\njulia> f = ifelse(x<0,NaN,sqrt(x))\n(ifelse  (x < 0) NaN sqrt(x))\n\njulia> g = make_function([f],[x])\n...\n\njulia> g([-1])\nERROR: DomainError with -1.0:\nsqrt was called with a negative real argument but will only return a complex result if called with a complex argument. Try sqrt(Complex(x)).\n\nHowever, you cannot yet compute derivatives of expressions that contain conditionals:\n\njulia> jacobian([f],[x,y])\nERROR: Your expression contained ifelse. FastDifferentiation does not yet support differentiation through ifelse or any of these conditionals (max, min, copysign, &, |, xor, <, >, <=, >=, !=, ==, signbit, isreal, iszero, isfinite, isnan, isinf, isinteger, !)\n\nThis may be a breaking change for some users. In previous versions the expression x==y returned a Bool. Some data structures, such as Dict use == by default to determine if two entries are the same. This will no longer work since x==y will now return an expression graph. Use an IdDict instead since this uses ===.\n\nA future PR will add support for differentiating through conditionals.","category":"section"}]
}
